{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Project 6:  Web Scraping\n",
    "### Finding Underpriced RVs on Craigslist\n",
    "\n",
    "![](https://snag.gy/WrdUMx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will be practicing our web scraping skills.  You can use Scrapy or Python requests in order to complete this project.  It may be helpful to write some prototype code in this notebook to test your assumptions, then move it into a Python file that can be run from the command line.\n",
    "\n",
    "> In order to run code from the command line, instead of the notebook, you just need to save your code to a file (with a .py extension), and run it using the Python interpreter:<br><br>\n",
    "> `python my_file.py`\n",
    "\n",
    "You will be building a process to scrape a single category of search results on Craigslist, that can easily be applied to other categories by changing the search terms.  The main goal is to be able to target and scrape a single page given a set of parameters.\n",
    "\n",
    "**If you use Scrapy, provide your code in a folder.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import your libraries for scrapy / requests / pandas / numpy / etc\n",
    "Setup whichever libraries you need. Review past material for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREPARE REQUIRED LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1.  Scrape for the largest US cities (non-exhaustive list)\n",
    "Search, research, and scrape Wikipedia for a list of the largest US cities.  There are a few sources but find one that is in a nice table.  We don't want all cities, just signifficant cities.  Examine your source.  Look for what can be differentiable.\n",
    "\n",
    "- Use requests\n",
    "- Build XPath query(ies)\n",
    "- Extract to a list\n",
    "- Clean your list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SCRAPE WIKIPEDIA FOR LARGEST US CITIES (NON-EXHAUSTIVE LIST)\n",
    "url = \"https://simple.wikipedia.org/wiki/List_of_United_States_cities_by_population\"\n",
    "response = requests.get(url)\n",
    "\n",
    "HTML = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cities = Selector(text=HTML).xpath('//tr/td[2]/a/text()').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'New York',\n",
       " u'Los Angeles',\n",
       " u'Chicago',\n",
       " u'Houston',\n",
       " u'Philadelphia',\n",
       " u'Phoenix',\n",
       " u'San Antonio',\n",
       " u'San Diego',\n",
       " u'Dallas',\n",
       " u'San Jose',\n",
       " u'Austin',\n",
       " u'Jacksonville',\n",
       " u'San Francisco',\n",
       " u'Indianapolis',\n",
       " u'Columbus',\n",
       " u'Fort Worth',\n",
       " u'Charlotte',\n",
       " u'Detroit',\n",
       " u'El Paso',\n",
       " u'Seattle',\n",
       " u'Denver',\n",
       " u'Washington',\n",
       " u'Memphis',\n",
       " u'Boston',\n",
       " u'Nashville',\n",
       " u'Baltimore',\n",
       " u'Oklahoma City',\n",
       " u'Portland',\n",
       " u'Las Vegas',\n",
       " u'Louisville',\n",
       " u'Milwaukee',\n",
       " u'Albuquerque',\n",
       " u'Tucson',\n",
       " u'Fresno',\n",
       " u'Sacramento',\n",
       " u'Long Beach',\n",
       " u'Kansas City',\n",
       " u'Mesa',\n",
       " u'Atlanta',\n",
       " u'Virginia Beach',\n",
       " u'Omaha',\n",
       " u'Colorado Springs',\n",
       " u'Raleigh',\n",
       " u'Miami',\n",
       " u'Oakland',\n",
       " u'Minneapolis',\n",
       " u'Tulsa',\n",
       " u'Cleveland',\n",
       " u'Wichita',\n",
       " u'New Orleans',\n",
       " u'Arlington',\n",
       " u'Bakersfield',\n",
       " u'Tampa',\n",
       " u'Aurora',\n",
       " u'Honolulu',\n",
       " u'Anaheim',\n",
       " u'Santa Ana',\n",
       " u'Corpus Christi',\n",
       " u'Riverside',\n",
       " u'St. Louis',\n",
       " u'Lexington',\n",
       " u'Pittsburgh',\n",
       " u'Stockton',\n",
       " u'Anchorage',\n",
       " u'Cincinnati',\n",
       " u'Saint Paul',\n",
       " u'Greensboro',\n",
       " u'Toledo',\n",
       " u'Newark',\n",
       " u'Plano',\n",
       " u'Henderson',\n",
       " u'Lincoln',\n",
       " u'Orlando',\n",
       " u'Jersey City',\n",
       " u'Chula Vista',\n",
       " u'Buffalo',\n",
       " u'Fort Wayne',\n",
       " u'Chandler',\n",
       " u'St. Petersburg',\n",
       " u'Laredo',\n",
       " u'Durham',\n",
       " u'Irvine',\n",
       " u'Madison',\n",
       " u'Norfolk',\n",
       " u'Lubbock',\n",
       " u'Gilbert',\n",
       " u'Winston\\u2013Salem',\n",
       " u'Glendale',\n",
       " u'Reno',\n",
       " u'Hialeah',\n",
       " u'Garland',\n",
       " u'Chesapeake',\n",
       " u'Irving',\n",
       " u'North Las Vegas',\n",
       " u'Scottsdale',\n",
       " u'Baton Rouge',\n",
       " u'Fremont',\n",
       " u'Richmond',\n",
       " u'Boise',\n",
       " u'San Bernardino',\n",
       " u'Birmingham',\n",
       " u'Spokane',\n",
       " u'Rochester',\n",
       " u'Modesto',\n",
       " u'Des Moines',\n",
       " u'Oxnard',\n",
       " u'Tacoma',\n",
       " u'Fontana',\n",
       " u'Fayetteville',\n",
       " u'Moreno Valley',\n",
       " u'Columbus',\n",
       " u'Huntington Beach',\n",
       " u'Yonkers',\n",
       " u'Montgomery',\n",
       " u'Aurora',\n",
       " u'Glendale',\n",
       " u'Shreveport',\n",
       " u'Akron',\n",
       " u'Little Rock',\n",
       " u'Amarillo',\n",
       " u'Augusta',\n",
       " u'Mobile',\n",
       " u'Grand Rapids',\n",
       " u'Salt Lake City',\n",
       " u'Huntsville',\n",
       " u'Tallahassee',\n",
       " u'Grand Prairie',\n",
       " u'Overland Park',\n",
       " u'Knoxville',\n",
       " u'Brownsville',\n",
       " u'Worcester',\n",
       " u'Newport News',\n",
       " u'Santa Clarita',\n",
       " u'Providence',\n",
       " u'Fort Lauderdale',\n",
       " u'Garden Grove',\n",
       " u'Oceanside',\n",
       " u'Rancho Cucamonga',\n",
       " u'Santa Rosa',\n",
       " u'Port St. Lucie',\n",
       " u'Chattanooga',\n",
       " u'Tempe',\n",
       " u'Jackson',\n",
       " u'Cape Coral',\n",
       " u'Vancouver',\n",
       " u'Ontario',\n",
       " u'Sioux Falls',\n",
       " u'Peoria',\n",
       " u'Springfield',\n",
       " u'Pembroke Pines',\n",
       " u'Elk Grove',\n",
       " u'Salem',\n",
       " u'Corona',\n",
       " u'Lancaster',\n",
       " u'Eugene',\n",
       " u'Palmdale',\n",
       " u'McKinney',\n",
       " u'Salinas',\n",
       " u'Fort Collins',\n",
       " u'Cary',\n",
       " u'Hayward',\n",
       " u'Springfield',\n",
       " u'Pasadena',\n",
       " u'Macon',\n",
       " u'Pomona',\n",
       " u'Alexandria',\n",
       " u'Escondido',\n",
       " u'Sunnyvale',\n",
       " u'Lakewood',\n",
       " u'Kansas City',\n",
       " u'Rockford',\n",
       " u'Torrance',\n",
       " u'Hollywood',\n",
       " u'Joliet',\n",
       " u'Bridgeport',\n",
       " u'Clarksville',\n",
       " u'Paterson',\n",
       " u'Naperville',\n",
       " u'Frisco',\n",
       " u'Mesquite',\n",
       " u'Savannah',\n",
       " u'Syracuse',\n",
       " u'Dayton',\n",
       " u'Pasadena',\n",
       " u'Orange',\n",
       " u'Fullerton',\n",
       " u'McAllen',\n",
       " u'Killeen',\n",
       " u'Hampton',\n",
       " u'Bellevue',\n",
       " u'Warren',\n",
       " u'Miramar',\n",
       " u'West Valley City',\n",
       " u'Olathe',\n",
       " u'Columbia',\n",
       " u'Sterling Heights',\n",
       " u'Thornton',\n",
       " u'New Haven',\n",
       " u'Waco',\n",
       " u'Charleston',\n",
       " u'Thousand Oaks',\n",
       " u'Visalia',\n",
       " u'Cedar Rapids',\n",
       " u'Elizabeth',\n",
       " u'Roseville',\n",
       " u'Gainesville',\n",
       " u'Carrollton',\n",
       " u'Stamford',\n",
       " u'Denton',\n",
       " u'Midland',\n",
       " u'Coral Springs',\n",
       " u'Concord',\n",
       " u'Topeka',\n",
       " u'Simi Valley',\n",
       " u'Surprise',\n",
       " u'Lafayette',\n",
       " u'Kent',\n",
       " u'Hartford',\n",
       " u'Santa Clara',\n",
       " u'Victorville',\n",
       " u'Abilene',\n",
       " u'Murfreesboro',\n",
       " u'Evansville',\n",
       " u'Vallejo',\n",
       " u'Athens',\n",
       " u'Allentown',\n",
       " u'Berkeley',\n",
       " u'Norman',\n",
       " u'Ann Arbor',\n",
       " u'Beaumont',\n",
       " u'Independence',\n",
       " u'Columbia',\n",
       " u'Springfield',\n",
       " u'El Monte',\n",
       " u'Fargo',\n",
       " u'Peoria',\n",
       " u'Provo',\n",
       " u'Lansing',\n",
       " u'Odessa',\n",
       " u'Downey',\n",
       " u'Wilmington',\n",
       " u'Arvada',\n",
       " u'Costa Mesa',\n",
       " u'Round Rock',\n",
       " u'Carlsbad',\n",
       " u'Miami Gardens',\n",
       " u'Westminster',\n",
       " u'Inglewood',\n",
       " u'Rochester',\n",
       " u'Fairfield',\n",
       " u'Elgin',\n",
       " u'West Jordan',\n",
       " u'Clearwater',\n",
       " u'Manchester',\n",
       " u'Lowell',\n",
       " u'Gresham',\n",
       " u'Cambridge',\n",
       " u'Ventura',\n",
       " u'Temecula',\n",
       " u'Waterbury',\n",
       " u'Antioch',\n",
       " u'Billings',\n",
       " u'High Point',\n",
       " u'Richardson',\n",
       " u'Richmond',\n",
       " u'West Covina',\n",
       " u'Pueblo',\n",
       " u'Murrieta',\n",
       " u'Centennial',\n",
       " u'Norwalk',\n",
       " u'North Charleston',\n",
       " u'Everett',\n",
       " u'Pompano Beach',\n",
       " u'Daly City',\n",
       " u'Palm Bay',\n",
       " u'Burbank',\n",
       " u'Wichita Falls',\n",
       " u'Boulder',\n",
       " u'Green Bay',\n",
       " u'Broken Arrow',\n",
       " u'West Palm Beach',\n",
       " u'College Station',\n",
       " u'Pearland',\n",
       " u'Santa Maria',\n",
       " u'El Cajon',\n",
       " u'San Mateo',\n",
       " u'Lewisville',\n",
       " u'Rialto',\n",
       " u'Davenport',\n",
       " u'Lakeland',\n",
       " u'Clovis',\n",
       " u'Edison',\n",
       " u'Sandy Springs',\n",
       " u'Tyler',\n",
       " u'Las Cruces',\n",
       " u'South Bend',\n",
       " u'Woodbridge']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 1.2 Only retain cities with properly formed ASCII\n",
    "\n",
    "Optionally, filter out any cities with impropper ASCII characters.  A smaller list will be easier to look at.  However you may not need to filter these if you spend more time scraping a more concise city list.  This list should help you narrow down the list of regional Craigslist sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cities = [city.encode('ascii',errors='ignore') for city in cities]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2.  Write a function to capture current pricing information via Craigslist in one city.\n",
    "Choose a city from your scraped data, then go to the cooresponding city section on Craigslist, searching for \"rv\" in the auto section.  Write a method that pulls out the prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 6896, 80, 1, 12500]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def price_grabber(city):\n",
    "    url = 'https://%s.craigslist.org/search/rva' % (city)\n",
    "    response = requests.get(url)\n",
    "    HTML = response.text\n",
    "    prices = Selector(text=HTML).xpath('//span[@class=\"l2\"]/span[@class=\"price\"]/text()').extract()\n",
    "    prices = [int(x.replace('$','')) for x in prices]\n",
    "    return prices\n",
    "    \n",
    "\n",
    "price_grabber(\"sfbay\")[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 2.1 Create a mapping of cities to cooresponding regional Craigslist URLs\n",
    "\n",
    "Major US cities on Craigslist typically have their own cooresponding section (ie: SFBay Area, NYC, Boston, Miami, Seattle, etc).  Later, you will use these to query search results for various metropolitian regions listed on Craigslist.  Between the major metropolitan Craigslist sites, the only thing that will differ is the URL's that correspond to them.\n",
    "\n",
    "The point of the \"mapping\":  Create a data structure that allows you to iterate with both the name of the city from Wikipedia, with the cooresponding variable that that will allow you to construct each craigslist URL for each region.\n",
    "\n",
    "> For San Francsico (the Bay Area metropolitan area), the url for the RV search result is:\n",
    "> http://sfbay.craigslist.org/search/sss?query=rv\n",
    ">\n",
    "> The convention is http://[region].craigslist.org/search/sss?query=rf\n",
    "> Replacing [region] with the cooresponding city name will allow you to quickly iterate through each regional Craigslist site, and scrape the prices from the search results.  Keep this in mind while you build this \"mapping\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['newyork',\n",
       " 'losangeles',\n",
       " 'chicago',\n",
       " 'houston',\n",
       " 'philadelphia',\n",
       " 'phoenix',\n",
       " 'sanantonio',\n",
       " 'sandiego',\n",
       " 'dallas',\n",
       " 'sanjose',\n",
       " 'austin',\n",
       " 'jacksonville',\n",
       " 'sanfrancisco',\n",
       " 'indianapolis',\n",
       " 'columbus',\n",
       " 'fortworth',\n",
       " 'charlotte',\n",
       " 'detroit',\n",
       " 'elpaso',\n",
       " 'seattle']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_list = [city.replace(' ', '').lower() for city in cities]\n",
    "\n",
    "cities_list = cities_list[:20] \n",
    "\n",
    "cities_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mapping(cities_list):\n",
    "    city_urls = {}\n",
    "    \n",
    "    for city in cities_list:\n",
    "        url = 'https://%s.craigslist.org/search/rva' %(city)\n",
    "        city_urls[city] = url\n",
    "    return city_urls\n",
    "\n",
    "city_map = mapping(cities_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'austin': 'https://austin.craigslist.org/search/rva',\n",
       " 'charlotte': 'https://charlotte.craigslist.org/search/rva',\n",
       " 'chicago': 'https://chicago.craigslist.org/search/rva',\n",
       " 'columbus': 'https://columbus.craigslist.org/search/rva',\n",
       " 'dallas': 'https://dallas.craigslist.org/search/rva',\n",
       " 'detroit': 'https://detroit.craigslist.org/search/rva',\n",
       " 'elpaso': 'https://elpaso.craigslist.org/search/rva',\n",
       " 'fortworth': 'https://fortworth.craigslist.org/search/rva',\n",
       " 'houston': 'https://houston.craigslist.org/search/rva',\n",
       " 'indianapolis': 'https://indianapolis.craigslist.org/search/rva',\n",
       " 'jacksonville': 'https://jacksonville.craigslist.org/search/rva',\n",
       " 'losangeles': 'https://losangeles.craigslist.org/search/rva',\n",
       " 'newyork': 'https://newyork.craigslist.org/search/rva',\n",
       " 'philadelphia': 'https://philadelphia.craigslist.org/search/rva',\n",
       " 'phoenix': 'https://phoenix.craigslist.org/search/rva',\n",
       " 'sanantonio': 'https://sanantonio.craigslist.org/search/rva',\n",
       " 'sandiego': 'https://sandiego.craigslist.org/search/rva',\n",
       " 'sanfrancisco': 'https://sanfrancisco.craigslist.org/search/rva',\n",
       " 'sanjose': 'https://sanjose.craigslist.org/search/rva',\n",
       " 'seattle': 'https://seattle.craigslist.org/search/rva'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 3. Define a function to caculate mean and median price per city.\n",
    "\n",
    "Now that you've created a list of cities you want to scrape, adapt your solution for grabbing data in one region site, to grab data for all regional sites that you collected, then calculate the mean and median price of RV results from each city.\n",
    "\n",
    "> Look at the URLs from a few different regions (ie: portland, phoenix, sfbay), and find what they have in common.  Determine the area in the URL string that needs to change the least, and figure out how to replace only that portion of the URL in order to iterate through each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def price_mean_median(city):\n",
    "    price = price_grabber(city)\n",
    "    return np.mean(price), np.median(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charlotte mean/median price: (3856.3225806451615, 1400.0)\n",
      "houston mean/median price: (18815.216216216217, 12800.0)\n",
      "columbus mean/median price: (17730.611111111109, 9200.0)\n",
      "losangeles mean/median price: (24409.976190476191, 8749.5)\n",
      "sanfrancisco mean/median price: (33517.948717948719, 23400.0)\n",
      "sandiego mean/median price: (19005.091954022988, 8999.0)\n",
      "phoenix mean/median price: (29140.166666666668, 9975.0)\n",
      "chicago mean/median price: (12518.46875, 7100.0)\n",
      "newyork mean/median price: (22117.010989010989, 10700.0)\n",
      "dallas mean/median price: (30884.114285714284, 21295.0)\n",
      "philadelphia mean/median price: (33407.558139534885, 14216.0)\n",
      "detroit mean/median price: (18675.023529411767, 14995.0)\n",
      "sanantonio mean/median price: (37086.816666666666, 19120.0)\n",
      "elpaso mean/median price: (60472.14736842105, 59995.0)\n",
      "indianapolis mean/median price: (15104.75, 11350.0)\n",
      "fortworth mean/median price:"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='fortworth.craigslist.org', port=443): Max retries exceeded with url: /search/rva (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x101bd6090>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-8d767a414dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcity_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean/median price:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_mean_median\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-57-4bbeab1a2665>\u001b[0m in \u001b[0;36mprice_mean_median\u001b[0;34m(city)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprice_mean_median\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprice_grabber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-e38204efbfd3>\u001b[0m in \u001b[0;36mprice_grabber\u001b[0;34m(city)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprice_grabber\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://%s.craigslist.org/search/rva'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mHTML\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//span[@class=\"l2\"]/span[@class=\"price\"]/text()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/requests/api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    466\u001b[0m         }\n\u001b[1;32m    467\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/requests/sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/requests/adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    435\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='fortworth.craigslist.org', port=443): Max retries exceeded with url: /search/rva (Caused by NewConnectionError('<requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x101bd6090>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))"
     ]
    }
   ],
   "source": [
    "for city in city_map:\n",
    "    print city, 'mean/median price:', price_mean_median(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 4. Run your scraping process, and save your results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 5. Do an analysis of the RV market.\n",
    "\n",
    "Go head we'll wait.  Anything notable about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"http://imgur.com/l5NasQj.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5.1 Does it makes sense to buy RVs in one region and sell them in another?\n",
    "\n",
    "Assuming the cost of shipping or driving from one regional market to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5.2 Can you pull out the \"make\" from the markup and include that in your analyis?\n",
    "How reliable is this data and does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "### 5.3 Are there any other variables you could pull out of the markup to help describe your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/xDpSobf.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 6. Move your project into scrapy (if you haven't used Scrapy yet)\n",
    "\n",
    ">Start a project by using the command `scrapy startproject [projectname]`\n",
    "> - Update your settings.py (review our past example)\n",
    "> - Update your items.py\n",
    "> - Create a spiders file in your `[project_name]/[project_name]/spiders` directory\n",
    "\n",
    "You can update your spider class with the complete list of craigslist \"start urls\" to effectively scrape all of the regions.  Start with one to test.\n",
    "\n",
    "Updating your parse method with the method you chose should require minimal changes.  It will require you to update your parse method to use the response parameter, and an item model (defined in items.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/GCAf1UX.png\" style=\"float: left; margin: 25px 15px 0px 0px; height: 25px\">\n",
    "\n",
    "## 7.  Chose another area of Craigslist to scrape.\n",
    "\n",
    "**Choose an area having more than a single page of results, then scrape multiple regions, multiple pages of search results and or details pages.**\n",
    "\n",
    "This is the true exercise of being able to understand how to succesffuly plan, develop, and employ a broader scraping strategy.  Even though this seems like a challenging task, a few tweeks of your current code can make this very managable if you've pieced together all the touch points.  If you are still confused as to some of the milestones within this process, this is an excellent opportunity to round out your understanding, or help you build a list of questions to fill in your gaps.\n",
    "\n",
    "_Use Scrapy!  Provide your code in this project directory when you submit this project._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

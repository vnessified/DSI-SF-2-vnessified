{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification model with very little data\n",
    "from this [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image as image_utils #????\n",
    "# from imagenet_utils import decode_predictions\n",
    "# from imagenet_utils import preprocess_input\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "- train small network from scratch (as baseline)\n",
    "- use bottleneck features of pre-trained network\n",
    "- fine-tune top layers of pre-trained network\n",
    "\n",
    "Keras features to cover:\n",
    "- `fit_generator` (for training model using python data generators)\n",
    "- `ImageDataGenerator` (for real-time data augmentation)\n",
    "- layer freezing and model fine-tuning\n",
    "\n",
    "Setup:\n",
    "- Keras, SciPy, PIL installed\n",
    "- NVIDIA GPU (with cuDNN installed) is nice (but not neccessary on small image set)\n",
    "- training + validation data directories with one subdirectly per image class (png or jpg)\n",
    "\n",
    "Deep learning on small-data problems:\n",
    "- deep learning requires learning features automatically from data, generally only possible when lots of training data is available esp where input samples are very high-dimensional like images\n",
    "- but convolutional neural networks one of best models for \"perceptual problems\" such as image classification even with little data to learn from\n",
    "- training convnets from scrath on small image dataset will still yield reasonable results\n",
    "- deep learning models are highly repurposable - you can take an image classification trained on large-scale data then reuse it on significantly different problem\n",
    "- many pre-trained models (usually trained on ImageNet dataset) can be use to bootstrap powerful vision models out of very little data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#path to images\n",
    "train_data_dir = '/Users/VanessaG/Desktop/pizza_class_data/train/'\n",
    "validation_data_dir = '/Users/VanessaG/Desktop/pizza_class_data/validation'\n",
    "\n",
    "#model paramter setup\n",
    "nb_train_samples = 2000\n",
    "nb_validation_samples = 800\n",
    "nb_epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#instantiate moel\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stack of 3 convolution layers with a ReLU activation and followed by max-pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dim_ordering: One of {\"th\", \"tf\"}. \"tf\" mode means that the images should have shape (samples, width, height, channels), \"th\" mode means that the images should have shape (samples, channels, width, height). It defaults to the image_dim_ordering value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be \"tf\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Because you're using tensorflow you have to put the \"depth\" as the 3rd dimension.\n",
    "# Apparently for theano the depth comes first like in tutorial (3, 150, 150)\n",
    "# and in tensorflow it comes last like (150, 150, 3)\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plus two fully-connected layers\n",
    "We end the model with a single unit and a sigmoid activation, which is perfect for a binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare data\n",
    "use .flow_from_directory() to generate batches of image data (and their labels) directly from our jpgs in their respective folders.\n",
    "\n",
    "### rescale\n",
    "- value by which you mulitply the data before any other processing\n",
    "- original image RGB coefficients 0-255 but these values are too high for our models to process (given a typical learning rate) so target values between 0-1 by scalling with a 1/255 factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# validation_generator = test_datagen.flow_from_directory(\n",
    "#         validation_data_dir,\n",
    "#         target_size=(img_width, img_height),\n",
    "#         batch_size=1,\n",
    "#         #save_to_dir='val_aug'\n",
    "#         class_mode='binary',\n",
    "#         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pizzas = glob.glob(validation_data_dir+'/pizza/*.jpg')\n",
    "# not_pizzas = glob.glob(validation_data_dir+'/not_pizza/*.jpg')\n",
    "# print len(pizzas), len(not_pizzas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# predicted_labels = []\n",
    "# for i, (img, y) in enumerate(validation_generator):\n",
    "#     pp = model.predict(img)\n",
    "#     label = int(pp[0] >= 0.5)\n",
    "#     predictions.append(pp[0][0])\n",
    "#     predicted_labels.append(label)\n",
    "#     if i >= 800:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print predictions[0:10]\n",
    "# print predicted_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.mean([1 if pl == l \n",
    "#          else 0 for pl, l \n",
    "#          in zip(predicted_labels, np.zeros(400).tolist()+np.ones(400).tolist())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_file_predictions = {}\n",
    "# for i, filename in enumerate(not_pizzas+pizzas):\n",
    "#     image_file_predictions[filename] = {\n",
    "#         'predicted_class':predicted_labels[i],\n",
    "#         'predicted_prob_pizza':predictions[i]\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# image_file_df = pd.DataFrame(image_file_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image_file_df.T.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use these generators to train model\n",
    "Fits the model on data generated batch-by-batch by a Python generator. The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 79s - loss: 0.6744 - acc: 0.5965 - val_loss: 0.6215 - val_acc: 0.6525\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.6228 - acc: 0.6775 - val_loss: 0.5397 - val_acc: 0.7238\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.5900 - acc: 0.7110 - val_loss: 0.5350 - val_acc: 0.7712\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.5570 - acc: 0.7390 - val_loss: 0.5325 - val_acc: 0.7350\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.5414 - acc: 0.7410 - val_loss: 0.7256 - val_acc: 0.5625\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.5407 - acc: 0.7380 - val_loss: 0.5973 - val_acc: 0.6713\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.5136 - acc: 0.7615 - val_loss: 0.4852 - val_acc: 0.7750\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.5182 - acc: 0.7495 - val_loss: 0.4416 - val_acc: 0.7975\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.5123 - acc: 0.7545 - val_loss: 0.6742 - val_acc: 0.6512\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.5051 - acc: 0.7640 - val_loss: 0.4468 - val_acc: 0.7987\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4887 - acc: 0.7750 - val_loss: 0.4515 - val_acc: 0.7963\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4727 - acc: 0.7790 - val_loss: 0.4368 - val_acc: 0.7987\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4829 - acc: 0.7790 - val_loss: 0.4437 - val_acc: 0.8063\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4753 - acc: 0.7850 - val_loss: 0.4299 - val_acc: 0.8150\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4696 - acc: 0.7845 - val_loss: 0.4320 - val_acc: 0.8025\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4594 - acc: 0.7930 - val_loss: 0.4073 - val_acc: 0.8225\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 5486s - loss: 0.4712 - acc: 0.7855 - val_loss: 0.6452 - val_acc: 0.6687\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 720s - loss: 0.4469 - acc: 0.7940 - val_loss: 0.3951 - val_acc: 0.8350\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4421 - acc: 0.8055 - val_loss: 0.4220 - val_acc: 0.8137\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4230 - acc: 0.8140 - val_loss: 0.4308 - val_acc: 0.8087\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4143 - acc: 0.8235 - val_loss: 0.4080 - val_acc: 0.8250\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4309 - acc: 0.8135 - val_loss: 0.4325 - val_acc: 0.8100\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4165 - acc: 0.8170 - val_loss: 0.4221 - val_acc: 0.8150\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4094 - acc: 0.8170 - val_loss: 0.3873 - val_acc: 0.8300\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.4095 - acc: 0.8135 - val_loss: 0.4140 - val_acc: 0.8250\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.4047 - acc: 0.8125 - val_loss: 0.4010 - val_acc: 0.8300\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3903 - acc: 0.8265 - val_loss: 0.3843 - val_acc: 0.8313\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3945 - acc: 0.8400 - val_loss: 0.4029 - val_acc: 0.8213\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3859 - acc: 0.8285 - val_loss: 0.4012 - val_acc: 0.8263\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3957 - acc: 0.8260 - val_loss: 0.4153 - val_acc: 0.8000\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3727 - acc: 0.8330 - val_loss: 0.4698 - val_acc: 0.8137\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3701 - acc: 0.8455 - val_loss: 0.4009 - val_acc: 0.8225\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 75s - loss: 0.3709 - acc: 0.8415 - val_loss: 0.3884 - val_acc: 0.8313\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3674 - acc: 0.8375 - val_loss: 0.7211 - val_acc: 0.7400\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3629 - acc: 0.8465 - val_loss: 0.4519 - val_acc: 0.8037\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 75s - loss: 0.3650 - acc: 0.8525 - val_loss: 0.4637 - val_acc: 0.8213\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 75s - loss: 0.3736 - acc: 0.8375 - val_loss: 0.4306 - val_acc: 0.8213\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3636 - acc: 0.8470 - val_loss: 0.3822 - val_acc: 0.8450\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3639 - acc: 0.8370 - val_loss: 0.5109 - val_acc: 0.7700\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3432 - acc: 0.8495 - val_loss: 0.3912 - val_acc: 0.8425\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3397 - acc: 0.8535 - val_loss: 0.3630 - val_acc: 0.8425\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3427 - acc: 0.8485 - val_loss: 0.3772 - val_acc: 0.8387\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3381 - acc: 0.8505 - val_loss: 0.3810 - val_acc: 0.8387\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3438 - acc: 0.8475 - val_loss: 0.3636 - val_acc: 0.8462\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 76s - loss: 0.3462 - acc: 0.8515 - val_loss: 0.3699 - val_acc: 0.8350\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3400 - acc: 0.8535 - val_loss: 0.3622 - val_acc: 0.8413\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 74s - loss: 0.3338 - acc: 0.8570 - val_loss: 0.3947 - val_acc: 0.8588\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3463 - acc: 0.8535 - val_loss: 0.3638 - val_acc: 0.8413\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3275 - acc: 0.8555 - val_loss: 0.4177 - val_acc: 0.8263\n",
      "Epoch 50/50\n",
      "2000/2000 [==============================] - 73s - loss: 0.3255 - acc: 0.8650 - val_loss: 0.4663 - val_acc: 0.7762\n"
     ]
    }
   ],
   "source": [
    "pizza_model = model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights('/Users/VanessaG/Desktop/DSI-SF-2-vnessified/capstone/keras_pizza_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc', 'loss', 'val_acc', 'val_loss']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pizza_model.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80617000000000005"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pizza_model.history['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(validation_generator, nb_validation_samples,\n",
    "                                      max_q_size=10, nb_worker=1, pickle_safe=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x123f19ed0>]], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEKCAYAAAARnO4WAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123f19650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "preds = pd.DataFrame(predictions)\n",
    "preds.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37268579304218291, 0.85250000000000004]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples, max_q_size=10,\n",
    "                         nb_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'not_pizza': 0, 'pizza': 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99488431],\n",
       "       [ 0.01543689],\n",
       "       [ 0.08674085],\n",
       "       [ 0.31872407],\n",
       "       [ 0.16791809],\n",
       "       [ 0.15143231],\n",
       "       [ 0.00232882],\n",
       "       [ 0.24438238],\n",
       "       [ 0.33933592],\n",
       "       [ 0.09753565]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = validation_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  0.,  0.,\n",
       "        0.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        1.,  1.,  0.,  1.,  1.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08126235],\n",
       "       [ 0.57426715],\n",
       "       [ 0.49691847],\n",
       "       [ 0.04579484],\n",
       "       [ 0.94406939],\n",
       "       [ 0.07271674],\n",
       "       [ 0.99968314],\n",
       "       [ 0.18125772],\n",
       "       [ 0.93656653],\n",
       "       [ 0.02479019],\n",
       "       [ 0.35465115],\n",
       "       [ 0.22954753],\n",
       "       [ 0.63639081],\n",
       "       [ 0.43296358],\n",
       "       [ 0.9122054 ],\n",
       "       [ 0.04420486],\n",
       "       [ 0.56888866],\n",
       "       [ 0.97104329],\n",
       "       [ 0.35707584],\n",
       "       [ 0.52468878],\n",
       "       [ 0.27928662],\n",
       "       [ 0.34185609],\n",
       "       [ 0.02060525],\n",
       "       [ 0.16323878],\n",
       "       [ 0.05697659],\n",
       "       [ 0.04494169],\n",
       "       [ 0.01118162],\n",
       "       [ 0.2705709 ],\n",
       "       [ 0.00876822],\n",
       "       [ 0.93954331],\n",
       "       [ 0.97481751],\n",
       "       [ 0.01143311]], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted_labels = [1 if p[0] > 0.5 else 0 for p in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48749999999999999"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(validation_generator.classes == predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32/32 [==============================] - 0s\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error when checking : expected convolution2d_input_3 to have 4 dimensions, but got array with shape (32, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-88cd98a3929e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimg_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         '''\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         x = standardize_input_data(x, self.input_names,\n\u001b[1;32m   1160\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                    check_batch_dim=False)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/VanessaG/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m     95\u001b[0m                                 \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                                 \u001b[0;34m' dimensions, but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                                 str(array.shape))\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error when checking : expected convolution2d_input_3 to have 4 dimensions, but got array with shape (32, 1)"
     ]
    }
   ],
   "source": [
    "predicted_classes = []\n",
    "for i, img_batch in enumerate(validation_generator):\n",
    "    print i\n",
    "    for img in img_batch:\n",
    "        predicted_classes.append(model.predict_classes(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tmp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
